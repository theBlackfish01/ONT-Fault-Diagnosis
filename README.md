# OTDR-ML Pipeline

End-to-end anomaly detection, fault diagnosis, and localisation for Optical-Time-Domain-Reflectometry traces.

---

## üìë Table of Contents

1. [Project Overview](#project-overview)
2. [Repository Structure](#repository-structure)
3. [Quick-Start](#quick-start)
4. [End-to-End Workflow](#end-to-end-workflow)

   * 4.1 üìä Data Preparation
   * 4.2 üîç Stage 1 ‚Äì GRU-AE Detector
   * 4.3 üîß Stage 2 ‚Äì TCN Diagnostician
   * 4.4 üìú Alternative ‚Äì Time-Series Transformer
   * 4.5 üìà Evaluation & Visualisation
   * 4.6 ü©∫ Model Explainability (SHAP + ChatGPT o3)
5. [Results](#results)
6. [Reproducing the Experiments](#reproducing-the-experiments)
7. [Dependencies](#dependencies)
8. [Acknowledgements](#acknowledgements)

---

## Project Overview

This repository implements and extends the two-stage pipeline from
*‚ÄúMachine-learning-based anomaly detection in optical fibre monitoring‚Äù*.

* **Stage 1 ‚Äì Detection.**
  A GRU-based Auto-Encoder (GRU-AE) flags anomalous windows by reconstruction error.
* **Stage 2 ‚Äì Diagnosis & Localisation.**
  *Primary* model: **Temporal Convolutional Network (TCN)**
  *Alternative* model: **Time-Series Transformer (TST)**

Additional modules provide **SHAP interpretability** and a **natural-language explanation** of SHAP output generated with *ChatGPT o3*.

---

## Repository Structure

```text
.
‚îú‚îÄ data/                     # not included in the repo, see below
‚îÇ  ‚îú‚îÄ OTDR_DATA.csv
‚îú‚îÄ models/                   # also not included in the repo, see below
‚îÇ  ‚îú‚îÄ gru_ae_cond.pt
‚îÇ  ‚îú‚îÄ gru_ae_deep.pt
‚îÇ  ‚îú‚îÄ best_tcn.pt
‚îÇ  ‚îî‚îÄ best_tst.pt
‚îú‚îÄ ONT_Models.ipynb          # Jupyter notebook for model training & evaluation
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md
```

---

## End-to-End Workflow

### 4.1 üìä Data Preparation

* **Input features**: `SNR`, `P1 ‚Ä¶ P30` (float32).
* **Targets**: `Class` (8 categories) and `Position` (m).
* **Split**: train 64 % / val 20 % / test 16 % (stratified on `Class`).
* **Normalisation**: `StandardScaler` fitted on train only, persisted to `feature_scaler.pkl`.

### 4.2 üîç Stage 1 ‚Äì GRU-AE Detector

| Parameter        | Value                       |
| ---------------- | --------------------------- |
| Layers (enc/dec) | 3                           |
| Hidden units     | 64                          |
| Latent dim       | 32                          |
| Optimiser        | Adam 1e-3                   |
| Threshold        | 0.029 MSE (94·µó ∞ percentile) |

### 4.3 üîß Stage 2 ‚Äì TCN Diagnostician

* **Architecture**: 6 dilated residual blocks (`k = 3`, dilations 1‚Ä¶32, 64 channels).
* **Loss**: `CE(fault) + 0.5 √ó MSE(position)`.
* **Training**: Adam 1e-3, gradient clipping, early stopping (patience 7).

### 4.4 üìú Alternative ‚Äì Time-Series Transformer

* 4-layer encoder-only Transformer, `d_model = 128`, 4 heads, dropout 0.1.
* Trained with AdamW 2e-4, LR step-decay.

### 4.5 üìà Evaluation & Visualisation


* GRU-AE ROC curve & error histogram
* Confusion matrices (TCN / TST)
* Per-class precision-recall bars
* Localisation parity scatter & error histograms


### 4.6 ü©∫ Model Explainability


1. Computes SHAP values (KernelSHAP) for the TCN on 1 000 random test windows.
2. Persists `tcn_shap_values.pkl`.
3. Sends a prompt + SHAP summary to *ChatGPT o3* via the OpenAI API.
4. Stores the **plain-language narrative** in `chatgpt_o3_narrative.md`.

---

## Results

| Metric            | TCN (direct) | TST (direct) | TST after GRU-AE |
| ----------------- |--------------|--------------|------------------|
| Accuracy          | **0.893**    | 0.882        | 0.961            |
| Macro F1          | 0.892        | 0.881        | 0.917            |
| Localisation RMSE | 0.038 m      | **0.028 m**  | 0.045m           |

> *Key finding:* Attention-based TST halves localisation error relative to the TCN, but the TCN edges ahead on classification accuracy. A hard AE ‚Üí TST cascade reduces throughput accuracy because detector recall = 90 %.

---


## Dependencies

* Python ‚â• 3.10
* PyTorch ‚â• 2.0
* scikit-learn, pandas, numpy
* matplotlib & seaborn
* shap
* tqdm
* openai ‚â• 1.3 *(only for SHAP narrative generation)*

A full, hash-pinned list is provided in `requirements.txt`.

---

## Acknowledgements

Original research inspiration: **M. Tourancheau et al.**, *‚ÄúMachine-learning-based anomaly detection in optical fibre monitoring,‚Äù* 2021.
SHAP library ¬© Scott Lundberg et al.
Natural-language explanations courtesy of **ChatGPT o3**.
